Great! Here's a complete script that does exactly what you asked for:

‚úÖ Tries to load the embedding model from local cache
‚úÖ If not found, downloads it once (online)
‚úÖ After that, always runs fully offline
‚úÖ Works with German or other languages
‚úÖ Uses HtmlRAG to process .html files and save chunk vectors


---

‚úÖ Final Script: htmlrag_vectorize_autodownload.py

import json
from pathlib import Path
from htmlrag import clean_html, build_block_tree
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoConfig

def chunk_blocks(blocks, tokenizer, min_tokens=400, max_tokens=600):
    chunks = []
    current_chunk = []
    current_token_count = 0

    for text, path, is_leaf in blocks:
        text = text.strip()
        if not text:
            continue

        token_count = len(tokenizer.tokenize(text))
        if current_token_count + token_count > max_tokens:
            if current_chunk:
                chunks.append(" ".join(current_chunk))
            current_chunk = [text]
            current_token_count = token_count
        else:
            current_chunk.append(text)
            current_token_count += token_count

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks


def html_to_vectors(html_path,
                    output_path="vectors.json",
                    embed_model_id="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                    lang="de",
                    min_tokens=400,
                    max_tokens=600):
    html_path = Path(html_path)
    if not html_path.exists():
        raise FileNotFoundError(f"HTML file not found: {html_path}")

    raw_html = html_path.read_text(encoding="utf-8")
    print(f"\nüìÑ Processing: {html_path.name}")

    # Step 1: Clean HTML
    cleaned_html = clean_html(raw_html, lang=lang)

    # Step 2: Block tree
    blocks, _ = build_block_tree(cleaned_html)
    print(f"üß± Found {len(blocks)} blocks")

    # Step 3: Load tokenizer (try offline, fallback to download)
    try:
        tokenizer = AutoTokenizer.from_pretrained(embed_model_id, local_files_only=True)
        print("‚úÖ Loaded tokenizer from cache")
    except:
        print("üåê Tokenizer not cached. Downloading...")
        tokenizer = AutoTokenizer.from_pretrained(embed_model_id)

    # Step 4: Chunk blocks
    chunks = chunk_blocks(blocks, tokenizer, min_tokens, max_tokens)
    print(f"‚úÇÔ∏è Chunked into {len(chunks)} segments")

    if not chunks:
        print("‚ö†Ô∏è No valid chunks to embed.")
        return

    # Step 5: Load embedding model (try offline, fallback to download)
    try:
        model = SentenceTransformer(embed_model_id, cache_folder=None)
        print("‚úÖ Loaded embedding model (cached or downloaded)")
    except Exception as e:
        raise RuntimeError(f"‚ùå Failed to load embedding model: {e}")

    # Step 6: Encode chunks
    vectors = model.encode(chunks)

    # Step 7: Save results
    results = [
        {"chunk_id": i, "text": chunk, "vector": vec.tolist()}
        for i, (chunk, vec) in enumerate(zip(chunks, vectors))
    ]

    Path(output_path).write_text(json.dumps(results, indent=2), encoding="utf-8")
    print(f"‚úÖ Saved vectors to {output_path}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Offline/auto-download HTML vectorizer for German and more")
    parser.add_argument("html_file", help="Path to input HTML file")
    parser.add_argument("--output", default="vectors.json", help="Output JSON file")
    parser.add_argument("--embed_model_id", default="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", help="Model ID or local path")
    parser.add_argument("--lang", default="de", help="Language code (e.g. de, en, fr)")
    parser.add_argument("--min_tokens", type=int, default=400, help="Minimum tokens per chunk")
    parser.add_argument("--max_tokens", type=int, default=600, help="Maximum tokens per chunk")

    args = parser.parse_args()

    html_to_vectors(
        html_path=args.html_file,
        output_path=args.output,
        embed_model_id=args.embed_model_id,
        lang=args.lang,
        min_tokens=args.min_tokens,
        max_tokens=args.max_tokens
    )


---

‚úÖ How It Works

First run (online): downloads model + tokenizer ‚Üí saved in cache

Later runs (offline): uses local_files_only=True ‚Üí no internet needed

‚úÖ Supports German, English, etc.

‚úÖ Free for commercial use (Apache 2.0 license)



---

‚úÖ To Use:

üì• First (online) run:

python htmlrag_vectorize_autodownload.py myfile.html --lang de

üì¥ Later (offline) runs:

# Works offline from cache
python htmlrag_vectorize_autodownload.py myfile.html --lang de


---

Let me know if you'd like:

An option to process a folder of HTML files

Output to .csv or .parquet

FAISS index builder for local search


You're all set for multilingual, offline embedding!

