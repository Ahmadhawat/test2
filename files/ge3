Perfect ‚Äî let me explain clearly and rewrite the full script with everything included inside, no need for external --lang, --output, or any command-line arguments.


---

‚úÖ Why We Used --lang?

The HtmlRAG toolkit supports optional language-specific HTML cleaning (e.g. keeping content tags that depend on the language). If you're only working with German, we can hardcode "de" and remove the need for --lang.


---

‚úÖ Your Final Script ‚Äî Clean, Simple, No Args

üìÑ htmlrag_vectorize_offline.py

import json
from pathlib import Path
from htmlrag import clean_html, build_block_tree
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer


def chunk_blocks(blocks, tokenizer, min_tokens=400, max_tokens=600):
    chunks = []
    current_chunk = []
    current_token_count = 0

    for text, path, is_leaf in blocks:
        text = text.strip()
        if not text:
            continue

        token_count = len(tokenizer.tokenize(text))
        if current_token_count + token_count > max_tokens:
            if current_chunk:
                chunks.append(" ".join(current_chunk))
            current_chunk = [text]
            current_token_count = token_count
        else:
            current_chunk.append(text)
            current_token_count += token_count

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks


def html_to_vectors(html_file_path):
    # üîß Configuration
    embed_model_id = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    lang = "de"
    min_tokens = 400
    max_tokens = 600
    output_path = "vectors.json"

    # üìÑ Read HTML file
    html_path = Path(html_file_path)
    if not html_path.exists():
        raise FileNotFoundError(f"‚ùå File not found: {html_path}")

    raw_html = html_path.read_text(encoding="utf-8")
    print(f"\nüìÑ Processing: {html_path.name}")

    # üßπ Step 1: Clean HTML
    cleaned_html = clean_html(raw_html, lang=lang)

    # üß± Step 2: Build block tree
    blocks, _ = build_block_tree(cleaned_html)
    print(f"üß± Found {len(blocks)} blocks")

    # ‚úÇÔ∏è Step 3: Chunking
    try:
        tokenizer = AutoTokenizer.from_pretrained(embed_model_id, local_files_only=True)
        print("‚úÖ Tokenizer loaded locally")
    except:
        print("üåê Tokenizer not found locally. Downloading...")
        tokenizer = AutoTokenizer.from_pretrained(embed_model_id)

    chunks = chunk_blocks(blocks, tokenizer, min_tokens, max_tokens)
    print(f"‚úÇÔ∏è Created {len(chunks)} chunks")

    if not chunks:
        print("‚ö†Ô∏è No valid text to embed.")
        return

    # üß† Step 4: Load embedding model
    try:
        model = SentenceTransformer(embed_model_id)
        print("‚úÖ Embedding model loaded")
    except Exception as e:
        raise RuntimeError(f"‚ùå Could not load model: {e}")

    # üîó Step 5: Embed
    vectors = model.encode(chunks)

    # üíæ Step 6: Save to JSON
    results = [
        {"chunk_id": i, "text": chunk, "vector": vector.tolist()}
        for i, (chunk, vector) in enumerate(zip(chunks, vectors))
    ]

    Path(output_path).write_text(json.dumps(results, indent=2), encoding="utf-8")
    print(f"‚úÖ Saved {len(results)} vectors to {output_path}")


# üîΩ Main Entry
if __name__ == "__main__":
    # üëá Edit this to match your HTML file
    html_file = "index.html"  # Just change this filename if needed

    html_to_vectors(html_file)


---

‚úÖ How to Use

1. Save the script as htmlrag_vectorize_offline.py


2. Put index.html (or any file) in the same folder


3. Run it:

python htmlrag_vectorize_offline.py



‚úÖ No arguments
‚úÖ Hardcoded for German
‚úÖ Downloads the model once, then runs offline forever


---

Let me know if you want to:

Convert the output to CSV

Automatically embed a whole folder of .html files

Add FAISS or Qdrant search


You're ready to run this cleanly now!



def chunk_blocks(blocks, tokenizer, min_tokens=400, max_tokens=600):
    chunks = []
    current_chunk = []
    current_token_count = 0

    for text, path, is_leaf in blocks:
        if text is None:
            continue
        text = text.strip()
        if not text:
            continue

        token_count = len(tokenizer.tokenize(text))
        if current_token_count + token_count > max_tokens:
            if current_chunk:
                chunks.append(" ".join(current_chunk))
            current_chunk = [text]
            current_token_count = token_count
        else:
            current_chunk.append(text)
            current_token_count += token_count

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks


















import json
from pathlib import Path
from htmlrag import clean_html, build_block_tree
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer


# ‚úÖ Safe string cleaner
def safe_strip(s):
    return s.strip() if isinstance(s, str) else ""


# ‚úÖ Chunk blocks into 400‚Äì600 token segments
def chunk_blocks(blocks, tokenizer, min_tokens=400, max_tokens=600):
    chunks = []
    current_chunk = []
    current_token_count = 0

    for block_text, path, is_leaf in blocks:
        block_text = safe_strip(block_text)
        if not block_text:
            continue

        token_count = len(tokenizer.tokenize(block_text))
        if current_token_count + token_count > max_tokens:
            if current_chunk:
                chunks.append(" ".join(current_chunk))
            current_chunk = [block_text]
            current_token_count = token_count
        else:
            current_chunk.append(block_text)
            current_token_count += token_count

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks


# ‚úÖ Main processing function
def html_to_vectors(html_file_path):
    # === Configuration ===
    embed_model_id = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    lang = "de"
    min_tokens = 400
    max_tokens = 600
    output_path = "vectors.json"

    # === Load HTML ===
    html_path = Path(html_file_path)
    if not html_path.exists():
        raise FileNotFoundError(f"‚ùå File not found: {html_path}")

    raw_html = html_path.read_text(encoding="utf-8")
    print(f"\nüìÑ Processing: {html_path.name}")

    # === Step 1: Clean HTML ===
    cleaned_html = clean_html(raw_html, lang=lang)

    # === Step 2: Build block tree ===
    blocks, _ = build_block_tree(cleaned_html)
    print(f"üß± Found {len(blocks)} blocks")

    # === Step 3: Load tokenizer ===
    try:
        tokenizer = AutoTokenizer.from_pretrained(embed_model_id, local_files_only=True)
        print("‚úÖ Tokenizer loaded from cache")
    except:
        print("üåê Tokenizer not cached. Downloading...")
        tokenizer = AutoTokenizer.from_pretrained(embed_model_id)

    # === Step 4: Chunk text ===
    chunks = chunk_blocks(blocks, tokenizer, min_tokens, max_tokens)
    print(f"‚úÇÔ∏è Chunked into {len(chunks)} segments")

    if not chunks:
        print("‚ö†Ô∏è No valid chunks to embed. Exiting.")
        return

    # === Step 5: Load model ===
    try:
        model = SentenceTransformer(embed_model_id)
        print("‚úÖ Embedding model loaded")
    except Exception as e:
        raise RuntimeError(f"‚ùå Could not load model: {e}")

    # === Step 6: Embed ===
    vectors = model.encode(chunks)

    # === Step 7: Save vectors ===
    results = [
        {"chunk_id": i, "text": chunk, "vector": vector.tolist()}
        for i, (chunk, vector) in enumerate(zip(chunks, vectors))
    ]

    Path(output_path).write_text(json.dumps(results, indent=2), encoding="utf-8")
    print(f"‚úÖ Saved {len(results)} vectors to {output_path}")


# ‚úÖ Run
if __name__ == "__main__":
    html_file = r"C:\Entwicklung\1AAAEntwicklung\KI\C1CH\RAG\htmlrag2\index.html"  # üîÅ Set your full HTML path here
    html_to_vectors(html_file)